{'url': 'https://www.semanticscholar.org/paper/TensorFlow%3A-A-system-for-large-scale-machine-Abadi-Barham/46200b99c40e8586c8a0f588488ab6414119fb28', 'title': b'TensorFlow: A system for large-scale machine learning', 'abstract': b'TLDR\nTensorFlow is a machine learning system that operates at large scale and in heterogeneous environments.\nAbstract TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications. Collapse'}

{'url': 'https://www.semanticscholar.org/paper/TensorFlow%3A-Large-Scale-Machine-Learning-on-Systems-Abadi-Agarwal/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d', 'title': b'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems', 'abstract': b'TLDR\nThis paper describes the TensorFlow interface for expressing machine learning algorithms, and an implementation of that interface that we have built at Google.\nAbstract TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org. Collapse'}

{'url': 'https://www.semanticscholar.org/paper/Data-Mining-Practical-Machine-Learning-Tools-and-%E0%B8%AA%E0%B8%B7%E0%B8%9A%E0%B8%AA%E0%B8%B4%E0%B8%87%E0%B8%AB%E0%B9%8C/b42b1bfdc262bf99e9484e2e9df94df216b96374', 'title': b'Data Mining Practical Machine Learning Tools and Techniques', 'abstract': ''}

{'url': 'https://www.semanticscholar.org/paper/Machine-learning-a-probabilistic-perspective-Murphy/25badc676197a70aaf9911865eb03469e402ba57', 'title': b'Machine learning - a probabilistic perspective', 'abstract': b"TLDR\nA comprehensive and self-contained introduction to the field of machine learning, based on a unified probabilistic approach.\nAbstract Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students. Collapse"}

{'url': 'https://www.semanticscholar.org/paper/Scikit-learn%3A-Machine-Learning-in-Python-Pedregosa-Varoquaux/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', 'title': b'Scikit-learn: Machine Learning in Python', 'abstract': b'TLDR\nScikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems, while maintaining an easy-to-use interface tightly integrated with the Python language.\nAbstract Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net. Collapse'}

{'url': 'https://www.semanticscholar.org/paper/Fashion-MNIST%3A-a-Novel-Image-Dataset-for-Machine-Xiao-Rasul/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', 'title': b'Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms', 'abstract': b'TLDR\nWe present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category.\nAbstract We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL Collapse'}

{'url': 'https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5', 'title': b'Neural Machine Translation by Jointly Learning to Align and Translate', 'abstract': b'TLDR\nNeural machine translation is a recently proposed approach to machine translation.\nAbstract Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. Collapse'}

{'url': 'https://www.semanticscholar.org/paper/Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams/82266f6103bade9005ec555ed06ba20b5210ff22', 'title': b'Gaussian Processes for Machine Learning', 'abstract': b'TLDR\nGaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines.\nAbstract Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes. Collapse'}

{'url': 'https://www.semanticscholar.org/paper/Convolutional-LSTM-Network%3A-A-Machine-Learning-for-Shi-Chen/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7', 'title': b'Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting', 'abstract': b'TLDR\nWe propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem.\nAbstract The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting. Collapse'}

{'url': 'https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e', 'title': b'Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation', 'abstract': b'TLDR\nIn this paper, we propose a novel neural network model called RNN Encoder\xe2\x80\x90 Decoder that consists of two recurrent neural networks (RNN).\nAbstract In this paper, we propose a novel neural network model called RNN Encoder\xe2\x80\x90 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\xe2\x80\x90Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. Collapse'}

