{"urls": ["https://www.semanticscholar.org/paper/TensorFlow%3A-A-system-for-large-scale-machine-Abadi-Barham/46200b99c40e8586c8a0f588488ab6414119fb28", "https://www.semanticscholar.org/paper/TensorFlow%3A-Large-Scale-Machine-Learning-on-Systems-Abadi-Agarwal/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "https://www.semanticscholar.org/paper/Practical-Bayesian-Optimization-of-Machine-Learning-Snoek-Larochelle/2e2089ae76fe914706e6fa90081a79c8fe01611e", "https://www.semanticscholar.org/paper/Fashion-MNIST%3A-a-Novel-Image-Dataset-for-Machine-Xiao-Rasul/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32", "https://www.semanticscholar.org/paper/Scikit-learn%3A-Machine-Learning-in-Python-Pedregosa-Varoquaux/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Convolutional-LSTM-Network%3A-A-Machine-Learning-for-Shi-Chen/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7", "https://www.semanticscholar.org/paper/Optimization-Methods-for-Large-Scale-Machine-Bottou-Curtis/d21703674ae562bae4a849a75847cdd9ead417df", "https://www.semanticscholar.org/paper/Adversarial-Machine-Learning-at-Scale-Kurakin-Goodfellow/e2a85a6766b982ff7c8980e57ca6342d22493827", "https://www.semanticscholar.org/paper/Towards-A-Rigorous-Science-of-Interpretable-Machine-Doshi-Velez-Kim/5c39e37022661f81f79e481240ed9b175dec6513", "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/3a60ce89788ebb10676ef12e8bd8935ffa0cba2f", "https://www.semanticscholar.org/paper/Machine-learning-and-the-physical-sciences-Carleo-Cirac/a9cbbef8f4426329d0687025b34287c35bdd8b38", "https://www.semanticscholar.org/paper/Machine-Learning-for-Fluid-Mechanics-Brunton-Noack/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5", "https://www.semanticscholar.org/paper/Towards-A-Rigorous-Science-of-Interpretable-Machine-Doshi-Velez-Kim/5c39e37022661f81f79e481240ed9b175dec6513", "https://www.semanticscholar.org/paper/A-Survey-on-Bias-and-Fairness-in-Machine-Learning-Mehrabi-Morstatter/3a60ce89788ebb10676ef12e8bd8935ffa0cba2f", "https://www.semanticscholar.org/paper/Machine-learning-and-the-physical-sciences-Carleo-Cirac/a9cbbef8f4426329d0687025b34287c35bdd8b38", "https://www.semanticscholar.org/paper/Machine-Learning-for-Fluid-Mechanics-Brunton-Noack/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5"], "pdf_urls": ["https://arxiv.org/pdf/1605.08695.pdf", "https://arxiv.org/pdf/1603.04467.pdf", "https://arxiv.org/pdf/1206.2944.pdf", "https://arxiv.org/pdf/1708.07747.pdf", "https://arxiv.org/pdf/1201.0490.pdf", "https://arxiv.org/pdf/1409.0473.pdf", "https://arxiv.org/pdf/1506.04214.pdf", "https://arxiv.org/pdf/1606.04838.pdf", "https://arxiv.org/pdf/1611.01236.pdf", "https://arxiv.org/pdf/1702.08608.pdf", "https://arxiv.org/pdf/1908.09635.pdf", "https://arxiv.org/pdf/1903.10563.pdf", "https://arxiv.org/pdf/1905.11075.pdf", "https://arxiv.org/pdf/1702.08608.pdf", "https://arxiv.org/pdf/1908.09635.pdf", "https://arxiv.org/pdf/1903.10563.pdf", "https://arxiv.org/pdf/1905.11075.pdf"], "titles": ["TensorFlow: A system for large-scale machine learning", "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "Practical Bayesian Optimization of Machine Learning Algorithms", "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms", "Scikit-learn: Machine Learning in Python", "Neural Machine Translation by Jointly Learning to Align and Translate", "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting", "Optimization Methods for Large-Scale Machine Learning", "Adversarial Machine Learning at Scale", "Towards A Rigorous Science of Interpretable Machine Learning", "A Survey on Bias and Fairness in Machine Learning", "Machine learning and the physical sciences", "Machine Learning for Fluid Mechanics", "Towards A Rigorous Science of Interpretable Machine Learning", "A Survey on Bias and Fairness in Machine Learning", "Machine learning and the physical sciences", "Machine Learning for Fluid Mechanics"], "abstracts": ["TLDR\nTensorFlow is a machine learning system that operates at large scale and in heterogeneous environments.\nAbstract TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications. Collapse", "TLDR\nThis paper describes the TensorFlow interface for expressing machine learning algorithms, and an implementation of that interface that we have built at Google.\nAbstract TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org. Collapse", "TLDR\nWe show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer.\nAbstract The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks. Collapse", "TLDR\nWe present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category.\nAbstract We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL Collapse", "TLDR\nScikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems, while maintaining an easy-to-use interface tightly integrated with the Python language.\nAbstract Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net. Collapse", "TLDR\nNeural machine translation is a recently proposed approach to machine translation.\nAbstract Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. Collapse", "TLDR\nWe propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem.\nAbstract The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting. Collapse", "TLDR\nThis paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications.\nAbstract This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations. Collapse", "TLDR\nAdversarial examples are malicious inputs designed to fool machine learning models.\nAbstract Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process. Collapse", "TLDR\nWe propose a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.\nAbstract As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning. Collapse", "TLDR\nWe review research investigating how biases in data skew what is learned by machine learning algorithms, and we listed different sources of biases that can affect AI applications.\nAbstract With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields. Collapse", "Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges. Collapse", "TLDR\nThis paper presents an overview of past history, current developments, and emerging opportunities of machine learning for fluid mechanics.\nAbstract The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from field measurements, experiments and large-scale simulations at multiple spatiotemporal scales. Machine learning offers a wealth of techniques to extract information from data that could be translated into knowledge about the underlying fluid mechanics. Moreover, machine learning algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of machine learning for fluid mechanics. It outlines fundamental machine learning methodologies and discusses their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experimentation, and simulation. Machine learning provides a powerful information processing framework that can enrich, and possibly even transform, current lines of fluid mechanics research and industrial applications. Collapse", "TLDR\nWe propose a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning. Expand", "TLDR\nWe review research investigating how biases in data skew what is learned by machine learning algorithms, and we listed different sources of biases that can affect AI applications. Expand", "Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges. Collapse", "TLDR\nThis paper presents an overview of past history, current developments, and emerging opportunities of machine learning for fluid mechanics. Expand"]}